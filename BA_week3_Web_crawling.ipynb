{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BA_week3_Web_crawling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1DYHtnfB-0gHr_cP5EwvAh9gf-JmPS70d",
      "authorship_tag": "ABX9TyPlA6+QYEhCanLyDpS8OG0t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeongmin-heo/Business-Analytics/blob/master/BA_week3_Web_crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBRnMoGom9jM"
      },
      "source": [
        "# 1. urllib\n",
        "\n",
        "- 파이썬은 웹 사이트에 있는 데이터를 추출하기 위해 urllib 라이브러리 사용\n",
        "- 이를 이용해 HTTP 또는 FTP를 사용해 데이터 다운로드 가능\n",
        "- urllib은 URL을 다루는 모듈을 모아 놓은 패키지\n",
        "- urllib.request 모듈은 웹 사이트에 있는 데이터에 접근하는 기능 제공, 또한 인증, 리다렉트, 쿠키처럼 인터넷을 이용한 다양한 요청과 처리가 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "513YHI41lQq8"
      },
      "source": [
        "from urllib import request #request를 실행하기 위한 라이브러리를 불러오는 과정"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Rjrv5SnGso"
      },
      "source": [
        "## 1.1. urllib.request를 이용한 다운로드\n",
        "- urllib.request 모듈에 있는 urlretrieve() 함수 이용\n",
        "- 다음의 코드는 PNG 파일을 test.png 라는 이름의 파일로 저장하는 예제임"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck7pT_5nnF-n",
        "outputId": "607890fb-c07b-460b-8616-37202f6ff1f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 라이브러리 읽어들이기 \n",
        "from urllib import request\n",
        "\n",
        "url=\"http://uta.pw/shodou/img/28/214.png\" #url불러오기\n",
        "savename=\"test.png\" #저장\n",
        "\n",
        "request.urlretrieve(url, savename) #request\n",
        "print(\"저장되었습니다\") #print"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "저장되었습니다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyqBt4b9nOhD"
      },
      "source": [
        "# 1.2. urlopen으로 파일에 저장하는 방법\n",
        "- request.urlopen()은 메모리에 데이터를 올린 후 파일에 저장하게 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAqEDK2SnNyC",
        "outputId": "7fbab676-7373-4b0c-fc6b-27e2c0e2048f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# URL과 저장경로 지정하기\n",
        "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
        "savename = \"test1.png\"\n",
        "#다운로드\n",
        "mem = request.urlopen(url).read()\n",
        "#파일로 저장하기, wb는 쓰기와 바이너리모드\n",
        "with open(savename, mode=\"wb\") as f:\n",
        "    f.write(mem)\n",
        "    print(\"저장되었습니다.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "저장되었습니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY2nOstynVzL"
      },
      "source": [
        "# 1.3. API 사용하기\n",
        "## 클라이언트 접속 정보 출력 (기본)\n",
        "- API는 사용자의 요청에 따라 정보를 반환하는 프로그램\n",
        "- IP 주소, UserAgent 등 클라이언트 접속정보 출력하는 \"IP 확인 API\" 접근해서 정보를 추출하는 프로그램\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK-Gug97nUJa",
        "outputId": "2f423a5b-acf5-4050-94d4-d3ff3524fb3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "#데이터 읽어들이기\n",
        "url=\"http://api.aoikujira.com/ip/ini\"\n",
        "res=request.urlopen(url)\n",
        "data=res.read()\n",
        "\n",
        "#바이너리를 문자열로 변환하기\n",
        "text=data.decode(\"utf-8\")\n",
        "print(text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ip]\n",
            "API_URI=http://api.aoikujira.com/ip/get.php\n",
            "REMOTE_ADDR=35.194.150.68\n",
            "REMOTE_HOST=68.150.194.35.bc.googleusercontent.com\n",
            "REMOTE_PORT=35740\n",
            "HTTP_HOST=api.aoikujira.com\n",
            "HTTP_USER_AGENT=Python-urllib/3.6\n",
            "HTTP_ACCEPT_LANGUAGE=\n",
            "HTTP_ACCEPT_CHARSET=\n",
            "SERVER_PORT=80\n",
            "FORMAT=ini\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EibOjDOOni_w"
      },
      "source": [
        "# 2. BeautifulSoup\n",
        "- 스크레이핑(Scraping or Crawling)이란 웹 사이트에서 데이터를 추출하고, 원하는 정보를 추출하는 것을 의미\n",
        "- BeautifulSoup란 파이썬으로 스크레이핑할 때 사용되는 라이브러리로서 HTML/XML에서 정보를 추출할 수 있도록 도와줌. 그러나 다운로드 기능은 없음.\n",
        "- 파이썬 라이브러리는 pip 명령어를 이용해 설치 가능. Python Package Index(PyPI)에 있는 패키지 명령어를 한줄로 설치 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa19OqJoniHB"
      },
      "source": [
        "from bs4 import BeautifulSoup #beautifulsoup 불러오기"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-jiJKvHnyNM"
      },
      "source": [
        "html = \"\"\"\n",
        "<html><body>\n",
        "  <h1>스크레이핑이란?</h1>\n",
        "  <p>웹 페이지를 분석하는 것</p>\n",
        "  <p>원하는 부분을 추출하는 것</p>\n",
        "</body></html>\n",
        "\"\"\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsGLmffVn08G"
      },
      "source": [
        "# 2.1. 기본 사용\n",
        "- 다음은 Beautifulsoup를 이용하여 웹사이트로부터 HTML을 가져와 문자열로 만들어 이용하는 예제임\n",
        "\n",
        "- h1 태그를 접근하기 위해 html-body-h1 구조를 사용하여 soup.html.body.h1 이런식으로 이용하게 됨.\n",
        "\n",
        "- p 태그는 두개가 있어 soup.html.body.p 한 후 next_sibling을 두번 이용하여 다음 p를 추출. 한번만 하면 그 다음 공백이 추출됨.\n",
        "- HTML 태그가 복잡한 경우 이런 방식으로 계속 진행하기는 적합하지 않음.\n",
        "\n",
        "## 2) HTML 분석하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJPTKckpn0Lj"
      },
      "source": [
        "soup = BeautifulSoup(html, 'html.parser') #html.parser를 불러옵니다."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMaPAooYn_ik"
      },
      "source": [
        "# 3) 원하는 부분 추출하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-E84TMen-14",
        "outputId": "fb81d24a-a441-4bf8-c3ad-591392b4d5fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "h1 = soup.html.body.h1 #body => h1를 h1로 저장\n",
        "p1 = soup.html.body.p #body => p를 p1로 저장\n",
        "p2 = p1.next_sibling.next_sibling "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<hr/>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIqtwDGnoEPF"
      },
      "source": [
        "# 4) 요소의 글자 출력하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myvfw-SxoDkR",
        "outputId": "99680743-91f7-4286-c618-bf49c5db2868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(f\"h1 = {h1.string}\") #h1의 string 을 print\n",
        "print(f\"p  = {p1.string}\") #p1의 string 을 print\n",
        "print(f\"p  = {p2.string}\") #p2의 string 을 print"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "h1 = 스크레이핑이란?\n",
            "p  = 웹 페이지를 분석하는 것\n",
            "p  = 원하는 부분을 추출하는 것\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY4ZXzrioI0w"
      },
      "source": [
        "# 2.2. 요소를 찾는 method\n",
        "## 단일 element 추출: find()\n",
        "- BeautifulSoup는 루트부터 하나하나 요소를 찾는 방법 말고도 find()라는 메소드를 제공함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUyEaxeyoHd1"
      },
      "source": [
        "soup = BeautifulSoup(html, 'html.parser') #html.parser를 불러옵니다."
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkpEvMnOoPsU"
      },
      "source": [
        "- 1) find() 메서드로 원하는 부분 추출하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0nfb6CUoMN_",
        "outputId": "2521b824-8ac2-41ac-8628-a4a872a29659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "title = soup.find(\"h1\") #h1 부분 추출\n",
        "body  = soup.find(\"p\") #p 부분 추출\n",
        "print(title)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<h1>스크레이핑이란?</h1>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obC8S6KLoULS"
      },
      "source": [
        "- 2) 텍스트 부분 출력하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISMfitL5oOrs",
        "outputId": "1aaec395-31c4-4a68-a7cb-cc4db91ebd5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(f\"#title = {title.string}\" ) #title의 string\n",
        "print(f\"#body = {body.string}\") #body의 string"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#title = 스크레이핑이란?\n",
            "#body = 웹 페이지를 분석하는 것\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uquJHuQNoZgE"
      },
      "source": [
        "## 복수 elements 추출: find_all()\n",
        "- 여러개의 태그를 한번에 추출하고자 할때 사용함. 다음의 예제에서는 여러개의 태그를 추출하는 법을 보여주고 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd870xh1oWpW"
      },
      "source": [
        "html = \"\"\"\n",
        "<html><body>\n",
        "  <ul>\n",
        "    <li><a href=\"http://www.naver.com\">naver</a></li>\n",
        "    <li><a href=\"http://www.daum.net\">daum</a></li>\n",
        "  </ul>\n",
        "</body></html>\n",
        "\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html, 'html.parser')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9fgsYSNoenH"
      },
      "source": [
        "- 1) find_all() 메서드로 추출하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N6fxCh8oc58",
        "outputId": "0484b0f3-38d6-4820-ec03-0a6c348f30a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "links = soup.find_all(\"a\") #find를 통해서 a의 모든 부분을 추출\n",
        "print(links, len(links))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<a href=\"http://www.naver.com\">naver</a>, <a href=\"http://www.daum.net\">daum</a>] 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH1yTEPYoj2w"
      },
      "source": [
        "- 2) 링크 목록 출력하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kepHyBiEohoO",
        "outputId": "3be1dcc5-2498-427b-a385-562d29b9dcba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "for a in links:\n",
        "    href = a.attrs['href'] # href의 속성에 있는 속성값을 추출\n",
        "    text = a.string \n",
        "    print(text, \">\", href)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "naver > http://www.naver.com\n",
            "daum > http://www.daum.net\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtGcdgiMonJl"
      },
      "source": [
        "# 3. Css Selector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGSJnhY8olay"
      },
      "source": [
        "html = \"\"\"\n",
        "<html><body>\n",
        "<div id=\"meigen\">\n",
        "  <h1>위키북스 도서</h1>\n",
        "  <ul class=\"items\">\n",
        "    <li>유니티 게임 이펙트 입문</li>\n",
        "    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
        "    <li>모던 웹사이트 디자인의 정석</li>\n",
        "  </ul>\n",
        "</div>\n",
        "</body></html>\n",
        "\"\"\"\n",
        "\n",
        "# HTML 분석하기 \n",
        "soup = BeautifulSoup(html, 'html.parser')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3oiUYrEouZk"
      },
      "source": [
        "- 필요한 부분을 CSS 쿼리로 추출하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yywoPwA9osWU",
        "outputId": "83a7b78c-9f79-4e3e-b443-8f7570335188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# 타이틀 부분 추출하기 --- (※3)\n",
        "h1 = soup.select_one(\"div#meigen > h1\").string\n",
        "print(f\"h1 = {h1}\")\n",
        "\n",
        "# 목록 부분 추출하기 --- (※4)\n",
        "li_list = soup.select(\"div#meigen > ul.items > li\")\n",
        "for li in li_list:\n",
        "  print(f\"li = {li.string}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "h1 = 위키북스 도서\n",
            "li = 유니티 게임 이펙트 입문\n",
            "li = 스위프트로 시작하는 아이폰 앱 개발 교과서\n",
            "li = 모던 웹사이트 디자인의 정석\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd6Njh3XowA6"
      },
      "source": [
        "# 4. 활용 예제\n",
        "앞서 배운 urllib과 BeautifulSoup를 조합하면, 웹스크레이핑 및 API 요청 작업을 쉽게 수행하실 수 있습니다.\n",
        "\n",
        "1. URL을 이용하여 웹으로부터 html을 읽어들임 (urllib)\n",
        "2. html 분석 및 원하는 데이터를 추출 (BeautifulSoup)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jmz6-Xxot6q"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib import request, parse"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z6ybzFuo34l"
      },
      "source": [
        "## 1) HTML 가져오기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEnJti9Co2Th"
      },
      "source": [
        "url = \"https://finance.naver.com/marketindex/\"\n",
        "res = request.urlopen(url) #url request"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIOWo8nBo6XU"
      },
      "source": [
        "## 2) HTML 분석하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_cYJxyNo5uc"
      },
      "source": [
        "soup = BeautifulSoup(res, \"html.parser\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9telqmuo9pM"
      },
      "source": [
        "## 3) 원하는 데이터 추출하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtqxvy3Ao9Fm",
        "outputId": "efa3ddbf-9beb-4545-9951-52136cda7a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "price = soup.select_one(\"div.head_info > span.value\").string #div.head_info > span.value 순서로 접근하여 string을 select\n",
        "print(\"usd/krw =\", price)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usd/krw = 1,175.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaOm39hEpCVf"
      },
      "source": [
        "# 4.2. 기상청 RSS\n",
        "## 1) HTML 가져오기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJDNHsa_pBN6",
        "outputId": "fa7ed4fe-29b4-48b9-f0b3-397e41a27d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
        "\n",
        "#매개변수를 URL로 인코딩한다.\n",
        "values = {\n",
        "    'stnId':'109'\n",
        "}\n",
        "\n",
        "params=parse.urlencode(values)\n",
        "url += \"?\"+params # URL에 매개변수 추가\n",
        "print(\"url=\", url)\n",
        "\n",
        "res = request.urlopen(url)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "url= http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVtO9PmOpH8D"
      },
      "source": [
        "## 2) HTML 분석하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tunlZohqpGhD"
      },
      "source": [
        "soup = BeautifulSoup(res, \"html.parser\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aHxPZPIpKQ2"
      },
      "source": [
        "## 3) 원하는 데이터 추출하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3abf5eLGpJjq",
        "outputId": "9a029999-83ad-4805-f190-283828e06697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "header = soup.find(\"header\") #header 찾기\n",
        "\n",
        "title = header.find(\"title\").text #title 찾기\n",
        "wf = header.find(\"wf\").text #wf 찾기\n",
        "\n",
        "print(title)\n",
        "print(wf)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "서울,경기도 육상중기예보\n",
            "○ (강수) 1일(목) 오후~2일(금) 오전에는 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 낮 기온은 20~25도로 오늘(25일, 24~27도)과 비슷하거나 조금 낮겠고, 아침 기온은 9~17도로 선선하겠습니다.<br />          특히, 내륙을 중심으로 낮과 밤의 기온차가 10도 내외로 크겠습니다.<br />○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rybv5sSGpNzn",
        "outputId": "53787965-394f-46e5-86f0-b27b2b30ce5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "title = soup.select_one(\"header > title\").text #header > title 순서로 접근하여 text 추출\n",
        "wf = header.select_one(\"wf\").text #header에서 wf접근하여 text추출\n",
        "\n",
        "print(title)\n",
        "print(wf)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "서울,경기도 육상중기예보\n",
            "○ (강수) 1일(목) 오후~2일(금) 오전에는 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 낮 기온은 20~25도로 오늘(25일, 24~27도)과 비슷하거나 조금 낮겠고, 아침 기온은 9~17도로 선선하겠습니다.<br />          특히, 내륙을 중심으로 낮과 밤의 기온차가 10도 내외로 크겠습니다.<br />○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjXkP3QlpQ50",
        "outputId": "e50202ca-398a-4444-f538-e367301f0994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 뒤의 인코딩 부분은 \"저자:윤동주\"라는 의미입니다.\n",
        "# 따로 입력하지 말고 위키 문헌 홈페이지에 들어간 뒤에 주소를 복사해서 사용하세요.\n",
        "\n",
        "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
        "res = request.urlopen(url)\n",
        "soup = BeautifulSoup(res, \"html.parser\")\n",
        "\n",
        "# #mw-content-text 바로 아래에 있는 \n",
        "# ul 태그 바로 아래에 있는\n",
        "# li 태그 아래에 있는\n",
        "# a 태그를 모두 선택합니다.\n",
        "a_list = soup.select(\"#mw-content-text   ul > li  a\")\n",
        "for a in a_list:\n",
        "    name = a.string\n",
        "    print(f\"- {name}\", )"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- 하늘과 바람과 별과 시\n",
            "- 증보판\n",
            "- 서시\n",
            "- 자화상\n",
            "- 소년\n",
            "- 눈 오는 지도\n",
            "- 돌아와 보는 밤\n",
            "- 병원\n",
            "- 새로운 길\n",
            "- 간판 없는 거리\n",
            "- 태초의 아침\n",
            "- 또 태초의 아침\n",
            "- 새벽이 올 때까지\n",
            "- 무서운 시간\n",
            "- 십자가\n",
            "- 바람이 불어\n",
            "- 슬픈 족속\n",
            "- 눈감고 간다\n",
            "- 또 다른 고향\n",
            "- 길\n",
            "- 별 헤는 밤\n",
            "- 흰 그림자\n",
            "- 사랑스런 추억\n",
            "- 흐르는 거리\n",
            "- 쉽게 씌어진 시\n",
            "- 봄\n",
            "- 참회록\n",
            "- 간(肝)\n",
            "- 위로\n",
            "- 팔복\n",
            "- 못자는밤\n",
            "- 달같이\n",
            "- 고추밭\n",
            "- 아우의 인상화\n",
            "- 사랑의 전당\n",
            "- 이적\n",
            "- 비오는 밤\n",
            "- 산골물\n",
            "- 유언\n",
            "- 창\n",
            "- 바다\n",
            "- 비로봉\n",
            "- 산협의 오후\n",
            "- 명상\n",
            "- 소낙비\n",
            "- 한난계\n",
            "- 풍경\n",
            "- 달밤\n",
            "- 장\n",
            "- 밤\n",
            "- 황혼이 바다가 되어\n",
            "- 아침\n",
            "- 빨래\n",
            "- 꿈은 깨어지고\n",
            "- 산림\n",
            "- 이런날\n",
            "- 산상\n",
            "- 양지쪽\n",
            "- 닭\n",
            "- 가슴 1\n",
            "- 가슴 2\n",
            "- 비둘기\n",
            "- 황혼\n",
            "- 남쪽 하늘\n",
            "- 창공\n",
            "- 거리에서\n",
            "- 삶과 죽음\n",
            "- 초한대\n",
            "- 산울림\n",
            "- 해바라기 얼굴\n",
            "- 귀뚜라미와 나와\n",
            "- 애기의 새벽\n",
            "- 햇빛·바람\n",
            "- 반디불\n",
            "- 둘 다\n",
            "- 거짓부리\n",
            "- 눈\n",
            "- 참새\n",
            "- 버선본\n",
            "- 편지\n",
            "- 봄\n",
            "- 무얼 먹구 사나\n",
            "- 굴뚝\n",
            "- 햇비\n",
            "- 빗자루\n",
            "- 기왓장 내외\n",
            "- 오줌싸개 지도\n",
            "- 병아리\n",
            "- 조개껍질\n",
            "- 겨울\n",
            "- 트루게네프의 언덕\n",
            "- 달을 쏘다\n",
            "- 별똥 떨어진 데\n",
            "- 화원에 꽃이 핀다\n",
            "- 종시\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRUKZuLjrO5Z"
      },
      "source": [
        "# 일반문제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv6-mQyyrLJ1"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib import request"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueoR3h9DrSR0"
      },
      "source": [
        "# 1. 네이버 뉴스 헤드라인\n",
        "- 배운 내용을 바탕으로 네이버 뉴스(https://news.naver.com/) 에서 헤드라인 뉴스의 제목을 추출해보고자 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVt4kXaWrRJs",
        "outputId": "ee4e2462-85f8-4b92-d63d-253b0cc9d08e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "url = \"https://news.naver.com/\"\n",
        "\n",
        "res = request.urlopen(url)\n",
        "soup = BeautifulSoup(res, \"html.parser\")\n",
        "\n",
        "selector = \"#today_main_news > div.hdline_news > ul > li > div.hdline_article_tit > a\"\n",
        "\n",
        "for a in soup.select(selector):\n",
        "    title = a.text\n",
        "    print(title)\n",
        "# HTTP Error 403: Forbidden 관계로 불러올수 없음"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-5e2aec0d08fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://news.naver.com/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 642\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM5L7Fk8rtfi"
      },
      "source": [
        "# 2. 시민의 소리 게시판"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFMFc0O3rWG_"
      },
      "source": [
        "url_head = \"https://www.sisul.or.kr\"\n",
        "\n",
        "url_board = url_head + \"/open_content/childrenpark/qna/qnaMsgList.do?pgno=1\"\n",
        "\n",
        "res = request.urlopen(url_board)\n",
        "soup = BeautifulSoup(res, \"html.parser\")\n",
        "\n",
        "# selector = \"#detail_con > div.generalboard > table > tbody > tr > td.left.title > a\"\n",
        "selector = \"#detail_con > div.generalboard > table > tbody > tr > td.left.title > a\" #selector의 접근 순서를 정의\n",
        "titles = []\n",
        "links = []\n",
        "for a in soup.select(selector): #순서대로 접근하여 \n",
        "    titles.append(a.text) #a의 text를 titles에 append\n",
        "    links.append(url_head + a.attrs[\"href\"]) #url_head와 연결하여 append\n",
        "    \n",
        "print(titles, links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqE1q95JrxMw"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "board_df = pd.DataFrame({\"title\": titles, \"link\": links})\n",
        "board_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8-sbMWLrzrw"
      },
      "source": [
        "board_df.to_csv(\"board.csv\", index=False) #csv로 저장"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}